{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Load Proccess**:\n",
    "\n",
    "**Objective**: This notebook shows the process of loading data into a sql database. For this, a free instance of [Render](https://github.com/DCajiao/workshop001_candidates_analysis/blob/main/docs/database/how_to_deploy_databases_on_render.md) will be used and the `merged_data` will be uploaded.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **First Step**: Load clean, processed and previously transformed data from a csv.\n",
    "\n",
    "Task:\n",
    "- Load the merge dataset. \n",
    "- Find insights with the functions of analysis_functions.py\n",
    "- What data types do we have?\n",
    "- What categorical variables do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the 'src' folder to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Remove the column display limit to show all columns in the DataFrame\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_df = pd.read_csv('../data/processed/merged_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 781 rows and 8 columns\n",
      "The columns are: ['Happiness_Rank', 'Country', 'Happiness_Score', 'GDP_per_capita', 'Freedom', 'Generosity', 'Perceptions_of_corruption', 'Year']\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {merged_data_df.shape[0]} rows and {merged_data_df.shape[1]} columns')\n",
    "print(f'The columns are: {merged_data_df.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Second Step**: Write the queries for the PostgreSQL database based on the data.\n",
    "\n",
    "Task:\n",
    "- Create the query to create the schema adjusted to the data we have (`sql/transformed_data_schema.sql`).\n",
    "- Create the query to upload the data seed we have (`sql/transformed_data_seed_data.sql`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pysqlschema import SQLSchemaGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Generating schema for transformed_data\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Infering SQL type for object\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for float64\n",
      "INFO:root:Infering SQL type for int64\n",
      "INFO:root:Query written to ../sql/transformed_data_schema.sql\n",
      "INFO:root:Generating seed data for transformed_data\n",
      "INFO:root:Query written to ../sql/transformed_data_seed_data.sql\n"
     ]
    }
   ],
   "source": [
    "generator = SQLSchemaGenerator(table_name='transformed_data')\n",
    "\n",
    "generator.generate_schema(merged_data_df, '../sql/transformed_data_schema.sql')\n",
    "generator.generate_seed_data(merged_data_df, '../sql/transformed_data_seed_data.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Third Step**: Upload data to database\n",
    "\n",
    "Task:\n",
    "- Import db class to use connector\n",
    "- Establish connection and execute the queries to create the schema and send the data.\n",
    "- Validate that the table has been created and that all records have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Database credentials loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the DB class\n",
    "from connections.db import DB\n",
    "db = DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tables sizes\n",
    "db.execute_query_file(\"../sql/queries/001_view_tables_sizes.sql\", fetch_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Remove the table if it already exists\n",
    "db.execute_query(\"DROP TABLE IF EXISTS transformed_data;\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Create schema\n",
    "db.execute_query_file(\"../sql/transformed_data_schema.sql\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    }
   ],
   "source": [
    "# Seed data\n",
    "db.execute_query_file(\"../sql/transformed_data_seed_data.sql\", fetch_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:✔ Connected to database\n",
      "INFO:root:✔ Query executed\n",
      "INFO:root:✔ Cursor closed\n",
      "INFO:root:✔ Connection closed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('public.transformed_data', 781)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the tables sizes\n",
    "db.execute_query_file(\"../sql/queries/001_view_tables_sizes.sql\", fetch_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop003-machine-learning-and-data-stre-WbIxUPDv-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
