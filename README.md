<p align="center"><img src="https://readme-typing-svg.herokuapp.com?font=Time+New+Roman&color=%23FFFFFF&size=25&center=true&vCenter=true&width=1000&height=100&lines=Workshop+3:+Machine+learning+and+Data+streaming+ğŸ§ ğŸš€"></a></p>

## ğŸ“‹ Overview  
This project builds an end-to-end pipeline for predicting happiness scores of countries using machine learning and real-time data streaming. It incorporates:
- ğŸ“Š **Exploratory Data Analysis (EDA)** for insights and transformations.
- ğŸ§  **Machine Learning** regression to predict happiness scores.
- ğŸš€ **Kafka-based streaming** for real-time processing.
- ğŸ—„ï¸ **Database integration** for storing predictions.
- ğŸ› ï¸ **API for on-demand predictions** hosted on a scalable platform.

The pipeline is modular and automated, enabling seamless updates and experimentation with models.

---

## ğŸ› ï¸ Tools and Technologies  
| **Technology**         | **Purpose**                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| ![Python](https://img.shields.io/badge/-Python-3776AB?logo=python&logoColor=white)        | Development and scripting                                              |
| ![Docker](https://img.shields.io/badge/-Docker-2496ED?logo=docker&logoColor=white)        | Containerization of services                                           |
| ![Kafka](https://img.shields.io/badge/-Apache%20Kafka-231F20?logo=apache-kafka&logoColor=white) | Real-time data streaming                                              |
| ![Airflow](https://img.shields.io/badge/-Apache%20Airflow-017CEE?logo=apache-airflow&logoColor=white)| Workflow orchestration and pipeline automation                        |
| ![PostgreSQL](https://img.shields.io/badge/-PostgreSQL-336791?logo=postgresql&logoColor=white)| Database for storing predictions                                       |
| ![Scikit-learn](https://img.shields.io/badge/-Scikit--learn-F7931E?logo=scikit-learn&logoColor=white)| Machine learning model development                                     |
| ![Matplotlib](https://img.shields.io/badge/-Matplotlib-11557C?logo=matplotlib&logoColor=white)       | Data visualization                                                     |
| ![Plotly](https://img.shields.io/badge/-Plotly-3F4F75?logo=plotly&logoColor=white)        | Advanced visualizations                                                |
| ![Pickle](https://img.shields.io/badge/-Pickle-5A9A34?logo=python&logoColor=white)       | Model serialization                                                    |
| ![Poetry](https://img.shields.io/badge/-Poetry-60A5FA?logo=python&logoColor=white)        | Dependency management and packaging                                    |

---

## ğŸ—ï¸ Project Nodes  

![diagram](./docs/diagrams/data-pipeline.png)

### 1. ğŸ“‚**Data Preprocessing**  
    - Receive the 5 csv
    - Performs data merging
    - Performs an EDA
    - Upload data by rendering schema and data seed queries

### 2. ğŸ” **EDA**  
    -  Analyzing variables such as GDP, Social Support, and Happiness Score.  
    - ğŸ“ˆ Visualizing trends and correlations.  

### 3. **Model Training**  

   - ğŸ¯ Training a regression model using a 70-30 split.  
   - ğŸ“Š Evaluating performance using RÂ² and other metrics.  

### 4. **Data Streaming**  
   - ğŸŒ€ Kafka producer streams transformed data in real time.  
   - âš™ï¸ Kafka consumer processes streamed data for predictions.  

### 5. **API**  
   - ğŸŒ Hosted API for predicting happiness scores.  
   - ğŸ” Automated redeployment via GitHub commits.  

### 6. **Database Management**  
   - ğŸ—„ï¸ PostgreSQL stores input features and predictions.  

### 7. **Pipeline Automation**  
   - ğŸ¤– Airflow orchestrates the ETL and automates model updates.  

---

## ğŸ“‚ Repository Contents  
```plaintext
api/
â”œâ”€â”€ ... # All the Happiness Prediction API. 
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ Workshop 3 Machine learning and Data streaming.pdf # Rubric of the project
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ ...  # All the files for Airflow Pipeline
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ api /
â”‚   â”‚   |   â”œâ”€â”€ Happiness Prediction API - Methods.postman_collection.json # API methods
â”‚   â”‚   â”œâ”€â”€ ...  # Copy of the documentation
â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”œâ”€â”€ consumer / # Kafka config for the consumer container
â”‚   â”‚   â”œâ”€â”€ producer / # Kafka config for the producer container
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ 00_happiness_score_prediction_model.pkl # Base model 
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ ... # Notebooks for all the proyect steps
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ queries /
â”‚   â”‚   â”œâ”€â”€ api /
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ connections /
â”‚   â”‚   â”œâ”€â”€ utils /
â”œâ”€â”€ Dockerfile            # Docker configuration for containerizing the Airflow
â”œâ”€â”€ docker-compose.yml    # docker-compose configuration
â”œâ”€â”€ pyproject.yml         # Poetry Config
â”œâ”€â”€ README.md                    # Documentation (you're reading it now!)
```

---

## ğŸš€ How to Run the Project  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/DCajiao/workshop003_Machine_learning_and_Data_streaming.git
cd workshop003_Machine_learning_and_Data_streaming
```

### 2ï¸âƒ£ Configure Environment Variables  
Create a `.env` file in `src/` and add the following:  
```plaintext
DBNAME=...
DBUSER=...
DBPASS=...
DBHOST=...
DBPORT=5432
```

### 3ï¸âƒ£ Start the Services  
Run the following commands:  
```bash
docker-compose up airflow-init
docker-compose up -d
```

### 4ï¸âƒ£ Access Airflow  to run the training pipeline
- Go to `http://localhost:8080`.  
- Log in with:
  - **Username**: `airflow`  
  - **Password**: `airflow`  
- Activate and trigger the DAG to execute the pipeline.  

### 5ï¸âƒ£ Access to consumer and producer container logs
- Watch in real time how data is sent from the *producer*
- Watch in real time how the *consumer* makes the request to the prediction API and inserts the features with the prediction into the database within the `predicted_data` table.

---
If you want to learn more about how this project works, you can find a more detailed analysis at [the final report](./docs/report/)


### ğŸŒŸ Enjoy exploring the automated happiness prediction pipeline! ğŸ˜Š  
